{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installing two packages\n"
      ],
      "metadata": {
        "id": "2NnQMjRBQpkq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rndeaDih_jeI",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install scikit-optimize\n",
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYr4n5rnV9J1"
      },
      "source": [
        "Setting up all the imports needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjQBxatAa7zP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import json\n",
        "import time\n",
        "import joblib\n",
        "import optuna\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,\n",
        "    learning_curve,\n",
        "    ParameterGrid,\n",
        "    cross_val_score\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error,\n",
        "    mean_absolute_error,\n",
        "    r2_score,\n",
        "    brier_score_loss,\n",
        "    average_precision_score,\n",
        "    precision_recall_curve\n",
        ")\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from scipy.special import logit, expit\n",
        "from scipy.sparse import hstack\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Integer, Real\n",
        "from skopt.callbacks import DeadlineStopper\n",
        "from scipy.special import expit\n",
        "from optuna.samplers import TPESampler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xppICUilVtcn"
      },
      "source": [
        "### Define Constants and File Paths\n",
        "\n",
        "This section defines the file paths for CVE datasets from both MITRE and NVD for the years 2022–2024, as well as the EPSS score archive. It sets global constants for the evaluation snapshot date, a random seed, and a runtime timeout. Before proceeding, the code verifies that all required data files are present in the working directory and prints the contents of the EPSS archive as a basic integrity check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gD4fhjLbVmKT"
      },
      "outputs": [],
      "source": [
        "ZIP22_MITRE = \"CVES_2022.zip\"\n",
        "ZIP22_NVD   = \"nvdcve-1.1-2022.zip\"\n",
        "ZIP23_MITRE = \"CVES_2023.zip\"\n",
        "ZIP23_NVD   = \"nvdcve-1.1-2023.zip\"\n",
        "ZIP24_MITRE = \"CVES_2024.zip\"\n",
        "ZIP24_NVD   = \"nvdcve-1.1-2024.zip\"\n",
        "\n",
        "EPSS_PATH = \"all_epss_scores.zip\"\n",
        "with zipfile.ZipFile(EPSS_PATH) as z:\n",
        "    print(\"Files in ZIP:\", z.namelist())\n",
        "\n",
        "SNAP_DATE = pd.Timestamp(\"2023-12-31\", tz=\"UTC\")\n",
        "SEED      = 45\n",
        "TIMEOUT   = 300         # timelimit\n",
        "\n",
        "# ensure all required files are present\n",
        "for p in (ZIP22_MITRE, ZIP22_NVD,cZIP23_MITRE, ZIP23_NVD, ZIP24_MITRE,\n",
        "          ZIP24_NVD, EPSS_PATH):\n",
        "    if not os.path.isfile(p):\n",
        "        raise FileNotFoundError(f\"Missing file: {p}\")\n",
        "\n",
        "global_start = time.perf_counter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxO4rtV6WVRz"
      },
      "source": [
        "### Extract EPSS scores\n",
        "\n",
        "This block extracts and loads EPSS scores from the `all_epss_scores.csv` file within the EPSS ZIP archive. It skips the two-line header, reads only the `cve` and `epss` columns, and filters the dataset to include only CVEs from the years 2022 to 2024. The number of filtered entries and loading time are printed for verification and debugging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UClaMmODWTzq"
      },
      "outputs": [],
      "source": [
        "t0 = time.perf_counter()\n",
        "\n",
        "with zipfile.ZipFile(EPSS_PATH) as z:\n",
        "    with z.open(\"all_epss_scores.csv\") as f:\n",
        "        epss_all = pd.read_csv(\n",
        "            f,\n",
        "            skiprows=2,\n",
        "            header=None,\n",
        "            names=[\"cve\", \"epss\", \"percentile\"]\n",
        "        )[[\"cve\", \"epss\"]]\n",
        "\n",
        "# Filter EPSS entries to only include CVEs from 2022 to 2024\n",
        "epss_all = epss_all[epss_all[\"cve\"].str.contains(\"CVE-202[2-4]-\")]\n",
        "print(f\"[DEBUG] Filtered EPSS (2022–2024): {len(epss_all)} rows\")\n",
        "\n",
        "t1 = time.perf_counter()\n",
        "print(f\"[1] EPSS loaded ({len(epss_all):,} rows) in {t1 - t0:.1f}s\")\n",
        "print(f\"Rows loaded: {len(epss_all):,}\")\n",
        "print(epss_all.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fhTj5cjWscP"
      },
      "source": [
        "### Load and Merge CVE Records\n",
        "\n",
        "Parses CVE metadata from a MITRE ZIP archive and enriches it with severity scores from an NVD ZIP archive and EPSS scores.\n",
        "\n",
        "The function proceeds in three stages:\n",
        "1. Lists the contents of the MITRE ZIP (for debugging purposes).\n",
        "2. Loads all CVE entries from the NVD JSON file and creates a fast lookup table by CVE ID.\n",
        "3. Iterates through each MITRE JSON file, extracting CVE metadata (ID, publication date, description, references, problem types) and CVSS v3 severity metrics from NVD if available.\n",
        "\n",
        "After collecting all valid records, the function returns a merged `DataFrame` containing only CVEs with corresponding EPSS scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6mFL_StWq3n"
      },
      "outputs": [],
      "source": [
        "def load_cves(mitre_zip_path, nvd_zip_path, epss_df):\n",
        "    recs = []\n",
        "\n",
        "    # Step 1: List MITRE ZIP contents (debug)\n",
        "    with zipfile.ZipFile(mitre_zip_path) as mitre_zip:\n",
        "        names = mitre_zip.namelist()\n",
        "        print(f\"[DEBUG] ZIP content in {mitre_zip_path} — total files: {len(names)}\")\n",
        "        for fn in names[:20]:\n",
        "            print(f\" - {fn}\")\n",
        "\n",
        "    # Step 2: Load NVD JSON from ZIP\n",
        "    with zipfile.ZipFile(nvd_zip_path) as nvd_zip:\n",
        "        with nvd_zip.open(nvd_zip.namelist()[0]) as f:\n",
        "            nvd_data = json.load(f)\n",
        "            nvd_lookup = {\n",
        "                item[\"cve\"][\"CVE_data_meta\"][\"ID\"]: item\n",
        "                for item in nvd_data[\"CVE_Items\"]\n",
        "            }\n",
        "\n",
        "    # Step 3: Iterate over MITRE JSON files inside the ZIP\n",
        "    with zipfile.ZipFile(mitre_zip_path) as mitre_zip:\n",
        "        for fn in mitre_zip.namelist():\n",
        "            # Skip non-json and __MACOSX files\n",
        "            if not fn.endswith(\".json\") or \"__MACOSX\" in fn:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                r = json.load(mitre_zip.open(fn))\n",
        "                m = r.get(\"cveMetadata\", {})\n",
        "                pub = m.get(\"datePublished\", \"\")\n",
        "                if not pub:\n",
        "                    continue\n",
        "\n",
        "                dt = pd.to_datetime(pub, utc=True)\n",
        "                cid = m.get(\"cveId\", \"\")\n",
        "                if not cid:\n",
        "                    print(f\"[WARN] No CVE ID in {fn}\")\n",
        "                    continue\n",
        "\n",
        "                cna = r.get(\"containers\", {}).get(\"cna\", {})\n",
        "                descs = cna.get(\"descriptions\", [])\n",
        "                desc = descs[0][\"value\"] if descs else \"\"\n",
        "                refs = cna.get(\"references\", [])\n",
        "                ptypes = cna.get(\"problemTypes\", [])\n",
        "\n",
        "                # Add NVD info if available\n",
        "                nvd = nvd_lookup.get(cid, {})\n",
        "                impact = nvd.get(\"impact\", {})\n",
        "                cvss_v3 = impact.get(\"baseMetricV3\", {}).get(\"cvssV3\", {})\n",
        "                score = cvss_v3.get(\"baseScore\", None)\n",
        "                severity = cvss_v3.get(\"baseSeverity\", None)\n",
        "\n",
        "                recs.append({\n",
        "                    \"cve\": cid,\n",
        "                    \"pub_dt\": dt,\n",
        "                    \"year\": dt.year,\n",
        "                    \"desc\": desc,\n",
        "                    \"desc_length\": len(desc),\n",
        "                    \"ref_count\": len(refs),\n",
        "                    \"cwe_count\": sum(len(pt.get(\"descriptions\", [])) for pt in ptypes),\n",
        "                    \"cvss_score\": score,\n",
        "                    \"cvss_severity\": severity,\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"[ERROR] Failed to parse {fn}: {e}\")\n",
        "\n",
        "    print(f\"[DEBUG] Loaded {len(recs)} CVEs from {mitre_zip_path}\")\n",
        "    df = pd.DataFrame(recs)\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"[WARN] No CVEs collected — returning empty DataFrame\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df_merged = df.merge(epss_df, on=\"cve\", how=\"inner\")\n",
        "    print(f\"[DEBUG] After EPSs merge: {len(df_merged)} records remain\")\n",
        "    return df_merged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq7_fjV9XEEo"
      },
      "source": [
        "### Prepare Training, Validation, and Test Sets from CVE Data\n",
        "\n",
        "This section loads CVE records from 2022 and 2023, filters to the top 5% by EPSS score, and samples 2,700 high-risk CVEs for modeling. The data is split into training (80%) and validation (20%) subsets using a fixed seed for reproducibility. Separately, 2024 CVEs are loaded, filtered using the same top-5% EPSS threshold, and a test set is sampled to match 20% of the training set size. This approach ensures the model is trained and evaluated on high-severity, high-risk examples from recent years."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HnfmJpbAW76p"
      },
      "outputs": [],
      "source": [
        "# 1. Load all CVEs from 2020–2023\n",
        "df22 = load_cves(ZIP22_MITRE, ZIP22_NVD, epss_all)\n",
        "df23 = load_cves(ZIP23_MITRE, ZIP23_NVD, epss_all)\n",
        "\n",
        "df_trainval_all = pd.concat([df22, df23], ignore_index=True)\n",
        "print(f\"Total training data (2020–2023): {len(df_trainval_all):,} CVEs\")\n",
        "\n",
        "# 2. Take top 5% by EPSS score\n",
        "epss_cutoff = df_trainval_all[\"epss\"].quantile(0.95)\n",
        "df_top5 = df_trainval_all[df_trainval_all[\"epss\"] >= epss_cutoff]\n",
        "print(f\"Top 5% data: {len(df_top5):,} CVEs with EPSS ≥ {epss_cutoff:.4f}\")\n",
        "\n",
        "# 3. Sample 2700 from top 5%\n",
        "df_sampled = df_top5.sample(n=2700, random_state=SEED)\n",
        "print(\"Sampled 2700 high-EPSS CVEs for modeling\")\n",
        "\n",
        "# 4. Split into train (80%) and val (20%)\n",
        "df_train, df_val = train_test_split(df_sampled, test_size=0.2, random_state=SEED)\n",
        "print(f\"Train size: {len(df_train):,} | Validation size: {len(df_val):,}\")\n",
        "\n",
        "# 5. Load 2024 and sample test set (20% of training size)\n",
        "df24 = load_cves(ZIP24_MITRE, ZIP24_NVD, epss_all)\n",
        "test_size = int(len(df_train) * 0.20)\n",
        "epss_cutoff_2024 = df24[\"epss\"].quantile(0.95)\n",
        "df_top5_2024 = df24[df24[\"epss\"] >= epss_cutoff_2024]\n",
        "df_test = df_top5_2024.sample(n=test_size, random_state=SEED)\n",
        "print(f\"Test size: {len(df_test):,} sampled from 2024 CVEs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpfSaLlLXpn2"
      },
      "source": [
        "### Featurize CVE Records and Prepare Inputs for Modeling\n",
        "\n",
        "This section prepares the CVE data for regression modeling by constructing both numerical and textual features. A new column, `days_since`, is computed to capture how many days have passed since each CVE's publication relative to a fixed snapshot date. For textual information, a TF-IDF vectorizer is fit on the CVE descriptions from the training set and then applied across all splits. This ensures that only training data informs the vocabulary, preventing any leakage from validation or test data.\n",
        "\n",
        "Alongside text features, several numeric features are extracted, including the CVE's year, description length, number of references, number of CWEs, and days since publication. These are concatenated with the TF-IDF vectors to form a combined sparse feature matrix for each split.\n",
        "\n",
        "The EPSS scores—originally in the \\([0,1]\\) probability range—are transformed into log-odds via the `logit` function to provide an unbounded target suitable for regression. Finally, the feature matrices are standardized using `StandardScaler` (with `with_mean=False` to accommodate sparse inputs), and both the vectorizer and scaler are saved to disk for reuse.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fVs1B7GgXr9H"
      },
      "outputs": [],
      "source": [
        "t0 = time.perf_counter()\n",
        "for d in (df_train, df_val, df_test):\n",
        "    d[\"days_since\"] = (SNAP_DATE - d[\"pub_dt\"]).dt.days\n",
        "\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=100, stop_words=\"english\")\n",
        "tfidf.fit(df_train[\"desc\"])\n",
        "joblib.dump(tfidf, \"tfidf_reg_bayes_tamed_timed.joblib\")\n",
        "\n",
        "num_cols = [\"year\",\"desc_length\",\"ref_count\",\"cwe_count\",\"days_since\"]\n",
        "def build_X_y(df):\n",
        "    Xn = df[num_cols].values\n",
        "    Xt = tfidf.transform(df[\"desc\"])\n",
        "    X  = hstack([Xn, Xt])\n",
        "    # Transform EPSS from [0,1] probability to log-odds (logit) scale for regression\n",
        "    y  = logit(df[\"epss\"].clip(1e-6, 1-1e-6))\n",
        "    return X, y\n",
        "\n",
        "X_tr, y_tr   = build_X_y(df_train)\n",
        "X_val, y_val = build_X_y(df_val)\n",
        "X_te, y_te   = build_X_y(df_test)\n",
        "print(f\" df_test: {df_test}\")\n",
        "\n",
        "\n",
        "scaler = StandardScaler(with_mean=False).fit(X_tr)\n",
        "X_tr_s = scaler.transform(X_tr)\n",
        "X_val_s= scaler.transform(X_val)\n",
        "X_te_s = scaler.transform(X_te)\n",
        "joblib.dump(scaler, \"scaler_reg_bayes_tamed_timed.joblib\")\n",
        "\n",
        "t1 = time.perf_counter()\n",
        "print(f\"[4] Featurization + scaling in {t1-t0:.1f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "756I-6iOXxqg"
      },
      "source": [
        "### Run Bayesian Optimization for XGBoost Hyperparameter Tuning\n",
        "\n",
        "This block performs hyperparameter tuning for an XGBoost regression model using Bayesian optimization via `BayesSearchCV`. A search space is defined over key XGBoost parameters such as tree depth, learning rate, regularization strength, and subsampling rates. The optimizer is configured to run for up to 50 iterations, using 5-fold cross-validation and negative mean squared error as the scoring metric.\n",
        "\n",
        "To keep optimization time bounded, a `DeadlineStopper` is used to enforce a strict timeout limit. The training data (`X_tr_s`, `y_tr`) is passed to the optimizer, which iteratively selects and evaluates hyperparameter configurations based on previous results. Once finished, the best parameters and the corresponding fitted model are extracted for use in downstream evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YNuDHVChXxGp"
      },
      "outputs": [],
      "source": [
        "search_spaces = {\n",
        "    \"max_depth\":         Integer(2, 6),\n",
        "    \"learning_rate\":     Real(0.1, 0.5, prior=\"uniform\"),\n",
        "    \"gamma\":             Real(0, 5, prior=\"uniform\"),\n",
        "    \"reg_lambda\":        Real(0, 5, prior=\"uniform\"),\n",
        "    \"reg_alpha\":         Real(0, 2, prior=\"uniform\"),\n",
        "    \"subsample\":         Real(0.7, 1.0, prior=\"uniform\"),\n",
        "    \"colsample_bytree\":  Real(0.7, 1.0, prior=\"uniform\")\n",
        "}\n",
        "\n",
        "print(\"[Bayes] Starting BayesSearchCV ...\")\n",
        "start_time = time.perf_counter()\n",
        "\n",
        "bayes = BayesSearchCV(\n",
        "    XGBRegressor(objective=\"reg:squarederror\", random_state=SEED),\n",
        "    search_spaces=search_spaces,\n",
        "    n_iter=50,\n",
        "    cv=5,\n",
        "    scoring=\"neg_mean_squared_error\",\n",
        "    n_jobs=-1,\n",
        "    random_state=SEED,\n",
        "    verbose=0,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "stopper = DeadlineStopper(total_time=TIMEOUT)\n",
        "bayes.fit(X_tr_s, y_tr, callback=[stopper])\n",
        "\n",
        "elapsed_time_bayes = time.perf_counter() - start_time\n",
        "\n",
        "print(f\"[Bayes] Done in {elapsed_time_bayes:.1f}s\")\n",
        "best_bayes_params = bayes.best_params_\n",
        "best_bayes_model = bayes.best_estimator_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV_j6-vFd0xd"
      },
      "source": [
        "### Manual Grid Search for Hyperparameter Tuning of XGBoost Regressor with Timeout\n",
        "\n",
        "This code block performs a manual grid search over a specified hyperparameter grid for an XGBoost regression model (XGBRegressor). It evaluates combinations of parameters such as max_depth, learning_rate, gamma, reg_lambda, reg_alpha, subsample, and colsample_bytree using 5-fold cross-validation based on the negative mean squared error metric. The search is time-limited by a defined TIMEOUT to prevent long-running computations. Throughout the process, progress and intermediate results are printed. Once the best parameter set is found within the time constraint, the model is retrained on the full training data using those optimal hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TnhxjHg3dzqP"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    \"max_depth\":         [2, 4, 6],\n",
        "    \"learning_rate\":     [0.1, 0.3, 0.5],\n",
        "    \"gamma\":             [0, 2.5, 5],\n",
        "    \"reg_lambda\":        [0, 2.5, 5],\n",
        "    \"reg_alpha\":         [0, 1, 2],\n",
        "    \"subsample\":         [0.7, 0.85, 1.0],\n",
        "    \"colsample_bytree\":  [0.7, 0.85, 1.0]\n",
        "}\n",
        "\n",
        "print(\"[Grid] Starting manual grid search with timeout of {} seconds...\".format(TIMEOUT))\n",
        "\n",
        "start_time = time.perf_counter()\n",
        "best_score = -np.inf\n",
        "best_params = None\n",
        "\n",
        "all_params = list(ParameterGrid(param_grid))\n",
        "total_combinations = len(all_params)\n",
        "\n",
        "for idx, params in enumerate(all_params, start=1):\n",
        "    elapsed_time = time.perf_counter() - start_time\n",
        "\n",
        "    if elapsed_time > TIMEOUT:\n",
        "        print(f\"    ⏱ Time limit reached ({elapsed_time:.0f}s); stopping grid search.\")\n",
        "        break\n",
        "\n",
        "    model = XGBRegressor(objective=\"reg:squarederror\", random_state=SEED, **params)\n",
        "    scores = cross_val_score(\n",
        "        model,\n",
        "        X_tr_s, y_tr,\n",
        "        cv=5,\n",
        "        scoring=\"neg_mean_squared_error\",\n",
        "        n_jobs=-1,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    mean_score = np.mean(scores)\n",
        "    print(f\"    {idx:4d}/{total_combinations:4d} params={params} → mean neg_MSE={mean_score:.4f} (elapsed {elapsed_time:.0f}s)\")\n",
        "\n",
        "    if mean_score > best_score:\n",
        "        best_score = mean_score\n",
        "        best_params = params\n",
        "\n",
        "elapsed_time_grid = time.perf_counter() - start_time\n",
        "print(f\"[Grid] Search done in {elapsed_time_grid:.0f}s; best_params={best_params}\")\n",
        "\n",
        "best_grid_model = XGBRegressor(objective=\"reg:squarederror\", random_state=SEED, **best_params)\n",
        "best_grid_model.fit(X_tr_s, y_tr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppiDczR5fHnc"
      },
      "source": [
        "### Hyperparameter Optimization of XGBoost Regressor Using TPE (Tree-structured Parzen Estimator)\n",
        "\n",
        "This code performs hyperparameter tuning for an XGBRegressor using Optuna’s TPE sampler, which is a Bayesian optimization method. It defines an objective function that suggests values for key model hyperparameters such as n_estimators, max_depth, learning_rate, subsample, colsample_bytree, reg_alpha, reg_lambda, and gamma. The model is evaluated with 5-fold cross-validation using mean squared error as the metric. The optimization runs with a time limit (TIMEOUT) and up to 50 trials, leveraging a progress bar for tracking. After finding the best hyperparameters, the model is retrained on the full training set with the optimized parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRtljETue3QS"
      },
      "outputs": [],
      "source": [
        "print(\"\\n[5-TPE] Starting TPE hyperparameter optimization...\")\n",
        "\n",
        "y_tr = y_tr.reset_index(drop=True)\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        \"n_estimators\":     trial.suggest_int(\"n_estimators\", 50, 300),\n",
        "        \"max_depth\":        trial.suggest_int(\"max_depth\", 2, 6),\n",
        "        \"learning_rate\":    trial.suggest_float(\"learning_rate\", 0.1, 0.5),\n",
        "        \"subsample\":        trial.suggest_float(\"subsample\", 0.7, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.7, 1.0),\n",
        "        \"reg_alpha\":        trial.suggest_float(\"reg_alpha\", 0.0, 2.0),\n",
        "        \"reg_lambda\":       trial.suggest_float(\"reg_lambda\", 0.0, 5.0),\n",
        "        \"gamma\":            trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
        "        \"random_state\":     SEED,\n",
        "        \"objective\":        \"reg:squarederror\"\n",
        "    }\n",
        "\n",
        "    model = XGBRegressor(**params)\n",
        "    cv = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "    val_scores = []\n",
        "\n",
        "    for train_idx, valid_idx in cv.split(X_tr_s):\n",
        "        model.fit(X_tr_s[train_idx], y_tr[train_idx])\n",
        "        preds = model.predict(X_tr_s[valid_idx])\n",
        "        val_scores.append(mean_squared_error(y_tr[valid_idx], preds))\n",
        "\n",
        "    return np.mean(val_scores)\n",
        "\n",
        "tpe_start = time.perf_counter()\n",
        "\n",
        "study = optuna.create_study(direction=\"minimize\", sampler=TPESampler(seed=SEED))\n",
        "study.optimize(objective, timeout=TIMEOUT, n_trials=50, show_progress_bar=True)\n",
        "\n",
        "tpe_end = time.perf_counter()\n",
        "elapsed_time_tpe = tpe_end - tpe_start\n",
        "\n",
        "print(f\"[5-TPE] TPE optimization finished in {elapsed_time_tpe:.1f}s\")\n",
        "print(f\"[5-TPE] Best hyperparameters found: {study.best_params}\")\n",
        "\n",
        "tpe_model = XGBRegressor(**study.best_params)\n",
        "tpe_model.fit(X_tr_s, y_tr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyAcRVzufew_"
      },
      "source": [
        "### Comprehensive Model Evaluation and Visualization for Multiple XGBoost Models\n",
        "\n",
        "This code block evaluates, compares, and visualizes the performance of several trained XGBoost models (best_bayes_model, best_grid_model, tpe_model) on different datasets: training, validation, and test sets.\n",
        "\n",
        "Evaluation Metrics:\n",
        "It defines an evaluate_model function to compute MSE, MAE, and R² scores, converting model outputs from logits to probabilities if needed.\n",
        "It then compiles results for each model and dataset into a Pandas DataFrame with a MultiIndex, including training time for each model.\n",
        "\n",
        "Output:\n",
        "The results table is printed and saved to CSV.\n",
        "Training times are visualized via a bar plot and saved as an image.\n",
        "\n",
        "Real-Scale Evaluation on Test Set:\n",
        "For the 2024 test dataset, predictions are transformed back to the original EPSS scale, and metrics plus Pearson correlation are calculated and displayed.\n",
        "Actual vs predicted EPSS values are saved, and scatter plots comparing these values are generated and saved for each model.\n",
        "\n",
        "Additional Visualization:\n",
        "Histograms show the distribution of EPSS values in both training and test datasets to give context on the target variable’s spread.\n",
        "\n",
        "This workflow provides a thorough performance summary, enabling easy comparison of models across multiple datasets and facilitating deeper insight through visualizations and saved reports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYwi5CJMfeJ-",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def evaluate_model(y_true, y_pred_logits):\n",
        "    # Convert model's logit predictions back to probability space\n",
        "    y_pred = expit(y_pred_logits)\n",
        "    y_true_real = expit(y_true) if np.max(y_true) > 1.0 else y_true\n",
        "    mse = mean_squared_error(y_true_real, y_pred)\n",
        "    mae = mean_absolute_error(y_true_real, y_pred)\n",
        "    r2 = r2_score(y_true_real, y_pred)\n",
        "    return mse, mae, r2\n",
        "\n",
        "models_ran = []\n",
        "elapsed_times = []\n",
        "results = []\n",
        "\n",
        "datasets = {\n",
        "    \"TRAIN\": (X_tr_s, y_tr),\n",
        "    \"VAL\": (X_val_s, y_val),\n",
        "    \"TEST\": (X_te_s, y_te),\n",
        "}\n",
        "\n",
        "# Check which models exist\n",
        "if 'best_bayes_model' in globals():\n",
        "    models_ran.append(\"Bayesian\")\n",
        "    elapsed_times.append(elapsed_time_bayes if 'elapsed_time_bayes' in globals() else np.nan)\n",
        "\n",
        "if 'best_grid_model' in globals():\n",
        "    models_ran.append(\"Grid\")\n",
        "    elapsed_times.append(elapsed_time_grid if 'elapsed_time_grid' in globals() else np.nan)\n",
        "\n",
        "if 'tpe_model' in globals():\n",
        "    models_ran.append(\"TPE\")\n",
        "    elapsed_times.append(elapsed_time_tpe if 'elapsed_time_tpe' in globals() else np.nan)\n",
        "\n",
        "# Create MultiIndex for model/dataset performance\n",
        "index = pd.MultiIndex.from_product([models_ran, datasets.keys()], names=[\"Model\", \"Dataset\"])\n",
        "df_results = pd.DataFrame(index=index, columns=[\"MSE\", \"MAE\", \"R2\", \"Time (s)\"])\n",
        "\n",
        "# Fill in evaluation metrics\n",
        "for model in models_ran:\n",
        "    model_obj = None\n",
        "    if model == \"Bayesian\":\n",
        "        model_obj = best_bayes_model\n",
        "    elif model == \"Grid\":\n",
        "        model_obj = best_grid_model\n",
        "    elif model == \"TPE\":\n",
        "        model_obj = tpe_model\n",
        "\n",
        "    for dataset_name, (X_data, y_true) in datasets.items():\n",
        "        y_pred = model_obj.predict(X_data)\n",
        "        mse, mae, r2 = evaluate_model(y_true, y_pred)\n",
        "\n",
        "        df_results.loc[(model, dataset_name), \"MSE\"] = mse\n",
        "        df_results.loc[(model, dataset_name), \"MAE\"] = mae\n",
        "        df_results.loc[(model, dataset_name), \"R2\"] = r2\n",
        "        if dataset_name == \"TRAIN\":\n",
        "            df_results.loc[(model, dataset_name), \"Time (s)\"] = elapsed_times[models_ran.index(model)]\n",
        "        else:\n",
        "            df_results.loc[(model, dataset_name), \"Time (s)\"] = \"\"\n",
        "\n",
        "print(\"\\n=== Model Evaluation Metrics ===\")\n",
        "print(df_results)\n",
        "\n",
        "# Save results to CSV\n",
        "df_results.to_csv(\"model_evaluation_metrics.csv\")\n",
        "print(\"Saved model evaluation metrics to 'model_evaluation_metrics.csv'\")\n",
        "\n",
        "# Bar plot: training times\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(models_ran, elapsed_times, color='skyblue')\n",
        "plt.ylabel(\"Time (seconds)\")\n",
        "plt.title(\"Model Training Time Comparison\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"model_training_time_comparison.png\")\n",
        "plt.show()\n",
        "print(\"Saved training time comparison plot to 'model_training_time_comparison.png'\")\n",
        "\n",
        "# Real Scale Evaluation on TEST Set\n",
        "print(\"\\n=== Real Scale Evaluation on TEST Set (2024) ===\")\n",
        "y_true_epss = df_test[\"epss\"].values\n",
        "\n",
        "for model in models_ran:\n",
        "    if model == \"Bayesian\":\n",
        "        model_obj = best_bayes_model\n",
        "    elif model == \"Grid\":\n",
        "        model_obj = best_grid_model\n",
        "    elif model == \"TPE\":\n",
        "        model_obj = tpe_model\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "    y_pred_logits = model_obj.predict(X_te_s)\n",
        "    y_pred_epss = expit(y_pred_logits)\n",
        "\n",
        "    mse_real = mean_squared_error(y_true_epss, y_pred_epss)\n",
        "    mae_real = mean_absolute_error(y_true_epss, y_pred_epss)\n",
        "    r2_real = r2_score(y_true_epss, y_pred_epss)\n",
        "    corr = np.corrcoef(y_true_epss, y_pred_epss)[0, 1]\n",
        "\n",
        "    print(f\"\\n[{model} Model — Real EPSS Comparison]\")\n",
        "    print(f\"MSE: {mse_real:.6f}\")\n",
        "    print(f\"MAE: {mae_real:.6f}\")\n",
        "    print(f\"Pearson Correlation: {corr:.4f}\")\n",
        "\n",
        "    # Save actual vs predicted values\n",
        "    results_df = pd.DataFrame({\n",
        "        \"actual_epss\": y_true_epss,\n",
        "        \"predicted_epss\": y_pred_epss\n",
        "    })\n",
        "    results_df.to_csv(f\"{model.lower()}_real_vs_predicted_epss.csv\", index=False)\n",
        "    print(f\"Saved real vs predicted EPSS to '{model.lower()}_real_vs_predicted_epss.csv'\")\n",
        "\n",
        "    # Scatter plot: actual vs predicted EPSS\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    plt.scatter(y_true_epss, y_pred_epss, alpha=0.4)\n",
        "    plt.plot([0, 1], [0, 1], 'r--', label=\"Ideal\")\n",
        "    plt.xlabel(\"Actual EPSS\")\n",
        "    plt.ylabel(\"Predicted EPSS\")\n",
        "    plt.title(f\"{model} Predictions vs Actual EPSS (2024 Test Set)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{model.lower()}_epss_scatter.png\")\n",
        "    plt.show()\n",
        "    print(f\"Saved scatter plot to '{model.lower()}_epss_scatter.png'\")\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "plt.hist(df_train[\"epss\"], bins=50)\n",
        "plt.title(\"EPSS value distribution in training data\")\n",
        "\n",
        "plt.hist(df_test[\"epss\"], bins=50)\n",
        "plt.title(\"EPSS value distribution in TEST data (2024)\")\n",
        "plt.xlabel(\"EPSS\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "df_test[\"epss\"].min()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring EPSS Score Distributions Across Training, Validation, and Test Sets\n",
        "\n",
        "This section provides a comprehensive statistical overview and visualization of the EPSS (Exploit Prediction Scoring System) scores across the training, validation, and test datasets. It begins by printing descriptive statistics for each dataset to summarize key distributional properties such as mean, median, and quartiles. These statistics are then consolidated into a single DataFrame, transposed for clarity, rounded for readability, and saved as a CSV file for further reference and reporting.\n",
        "\n",
        "To visually compare the EPSS distributions, overlaid histograms display the score frequencies across all three datasets, highlighting differences and similarities in their distributions. A dedicated histogram focuses on the test set’s 2024 data, providing a detailed view of its EPSS score spread. The unique counts of CVEs within each dataset are also reported to contextualize the scope of the data involved.\n",
        "\n",
        "Further analysis explores the distribution of EPSS scores within the combined training and validation data by calculating key quantiles—specifically, the 75th and 95th percentiles—to isolate the top 25% and top 5% of vulnerabilities by risk score. Separate histograms with kernel density estimates illustrate the distribution of EPSS scores for the full dataset as well as these high-risk subsets. Each plot is clearly titled, saved, and formatted to support a nuanced understanding of how the scores are distributed across different risk strata, thereby enhancing insight into the data’s behavior and informing downstream modeling or decision-making efforts."
      ],
      "metadata": {
        "id": "4oX1_QJATfNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Set EPSS Statistics:\")\n",
        "print(df_train[\"epss\"].describe())\n",
        "\n",
        "print(\"\\nValidation Set EPSS Statistics:\")\n",
        "print(df_val[\"epss\"].describe())\n",
        "\n",
        "print(\"\\nTest Set EPSS Statistics:\")\n",
        "print(df_test[\"epss\"].describe())\n",
        "print(\"\\n\")\n",
        "\n",
        "# Compute basic EPSS statistics for each dataset\n",
        "stats = {\n",
        "    \"Train\": df_train[\"epss\"].describe(),\n",
        "    \"Validation\": df_val[\"epss\"].describe(),\n",
        "    \"Test (2024)\": df_test[\"epss\"].describe()\n",
        "}\n",
        "\n",
        "# Convert to a DataFrame\n",
        "df_epss_stats = pd.DataFrame(stats)\n",
        "\n",
        "# Transpose to make datasets rows and statistics columns\n",
        "df_epss_stats = df_epss_stats.T\n",
        "\n",
        "# Round for readability\n",
        "df_epss_stats = df_epss_stats.round(4)\n",
        "\n",
        "# Save to CSV\n",
        "df_epss_stats.to_csv(\"epss_summary_statistics.csv\")\n",
        "\n",
        "# Show it\n",
        "print(\"Saved EPSS summary statistics to 'epss_summary_statistics.csv'\")\n",
        "print(df_epss_stats)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(df_train[\"epss\"], bins=40, alpha=0.6, label=\"Train\", color='blue')\n",
        "plt.hist(df_val[\"epss\"], bins=40, alpha=0.6, label=\"Validation\", color='green')\n",
        "plt.hist(df_test[\"epss\"], bins=40, alpha=0.6, label=\"Test (2024)\", color='orange')\n",
        "plt.xlabel(\"EPSS Score\")\n",
        "plt.ylabel(\"Number of CVEs\")\n",
        "plt.title(\"EPSS Score Distribution Across Datasets\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"epss_distribution_all_datasets.png\")\n",
        "plt.show()\n",
        "\n",
        "plt.hist(df_test[\"epss\"], bins=50, color='skyblue')\n",
        "plt.title(\"EPSS Score Distribution in Test Set (2024)\")\n",
        "plt.xlabel(\"EPSS Score\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"epss_distribution_test_2024.png\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Training CVEs: {df_train['cve'].nunique()}\")\n",
        "print(f\"Validation CVEs: {df_val['cve'].nunique()}\")\n",
        "print(f\"Test CVEs (2024): {df_test['cve'].nunique()}\")\n",
        "\n",
        "# Ensure EPS values are in a numpy array\n",
        "epss_values = df_trainval_all[\"epss\"].values\n",
        "\n",
        "# Compute 25% and 5% quantile thresholds\n",
        "q25 = np.quantile(epss_values, 0.75)\n",
        "q95 = np.quantile(epss_values, 0.95)\n",
        "\n",
        "# Create datasets\n",
        "df_100 = df_trainval_all.copy()\n",
        "df_25 = df_trainval_all[df_trainval_all[\"epss\"] >= q25]\n",
        "df_5  = df_trainval_all[df_trainval_all[\"epss\"] >= q95]\n",
        "\n",
        "# Dataset info for captions\n",
        "total_scores_100 = len(df_100)\n",
        "total_scores_25 = len(df_25)\n",
        "total_scores_5 = len(df_5)\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# --- Plot 1: Full dataset ---\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.histplot(df_100[\"epss\"], kde=True, bins=30, color=\"skyblue\")\n",
        "plt.title(f\"Total Scores: {total_scores_100}\")\n",
        "plt.xlabel(\"EPSS Score\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"epss_distribution_full_dataset.png\")\n",
        "plt.show()\n",
        "\n",
        "# --- Plot 2: Top 25% ---\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.histplot(df_25[\"epss\"], kde=True, bins=30, color=\"orange\")\n",
        "plt.title(f\"Total Scores: {total_scores_25}\")\n",
        "plt.xlabel(\"EPSS Score\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"epss_distribution_top_25_percent.png\")\n",
        "plt.show()\n",
        "\n",
        "# --- Plot 3: Top 5% ---\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.histplot(df_5[\"epss\"], kde=True, bins=30, color=\"red\")\n",
        "plt.title(f\"Total Scores: {total_scores_5}\")\n",
        "plt.xlabel(\"EPSS Score\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"epss_distribution_top_5_percent.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hdosBOLwThgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xYN2GvpdKLHj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}